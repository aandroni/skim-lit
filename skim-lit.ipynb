{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipt1WLy5J16w"
   },
   "source": [
    "# SkimLit\n",
    "\n",
    "NLP model to make reading medical abstracts easier.\n",
    "\n",
    "To do this, we will implement the deep learning model described in https://arxiv.org/pdf/1612.05251.pdf\n",
    "\n",
    "Project done on Google Colab (Tesla T4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqRWqPs6KVJp"
   },
   "source": [
    "## Get the data\n",
    "\n",
    "The data is publicly available on GitHub so let's download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQhWcP2tKJJA",
    "outputId": "de88e239-dd16-47e2-f092-4b347d283ced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pubmed-rct'...\n",
      "remote: Enumerating objects: 30, done.\u001b[K\n",
      "remote: Total 30 (delta 0), reused 0 (delta 0), pack-reused 30\u001b[K\n",
      "Unpacking objects: 100% (30/30), done.\n",
      "PubMed_200k_RCT\n",
      "PubMed_200k_RCT_numbers_replaced_with_at_sign\n",
      "PubMed_20k_RCT\n",
      "PubMed_20k_RCT_numbers_replaced_with_at_sign\n",
      "README.md\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct\n",
    "!ls pubmed-rct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UczyWLTUK-UT",
    "outputId": "6453cd8c-3027-4153-c521-c40763adea40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.txt  test.txt  train.txt\n"
     ]
    }
   ],
   "source": [
    "!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have short computing times, we will use the PubMed_20k_RCT dataset, which is a subset (10%) of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1wiF_QFLLTh"
   },
   "outputs": [],
   "source": [
    "# Let's start working on the smaller dataset (20k instead of 200k)\n",
    "data_dir = \"/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3TVslibL9VE",
    "outputId": "69e0982a-9b20-4882-a00d-c35dababb79b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt',\n",
       " '/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt',\n",
       " '/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjmHe-uxMYcT"
   },
   "source": [
    "## Preprocess the data\n",
    "\n",
    "For the deep learning model to run, we will need to transform text into numbers. Hovewer, before this preprocessing step, let's see what the data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duyRXLKoMEDk"
   },
   "outputs": [],
   "source": [
    "# Create function to read text\n",
    "def get_lines(filename):\n",
    "  \"\"\"\n",
    "  Reads filename and returns the lines as a list.\n",
    "\n",
    "  Args:\n",
    "    filename: path to text file.\n",
    "\n",
    "  Returns:\n",
    "    A list of strings with one string per line.\n",
    "  \"\"\"\n",
    "  with open(filename, \"r\") as ifs:\n",
    "    return ifs.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pFclj57WM-oS",
    "outputId": "b277ef8d-e96f-416a-87eb-73e747fdd133"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['###24293578\\n',\n",
       " 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n',\n",
       " 'METHODS\\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n',\n",
       " 'METHODS\\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\\n',\n",
       " 'METHODS\\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\\n',\n",
       " 'METHODS\\tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .\\n',\n",
       " 'METHODS\\tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .\\n',\n",
       " 'RESULTS\\tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .\\n',\n",
       " 'RESULTS\\tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .\\n',\n",
       " 'RESULTS\\tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .\\n',\n",
       " 'RESULTS\\tThese differences remained significant at @ weeks .\\n',\n",
       " 'RESULTS\\tThe Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .\\n',\n",
       " 'CONCLUSIONS\\tLow-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee OA ( ClinicalTrials.gov identifier NCT@ ) .\\n',\n",
       " '\\n',\n",
       " '###24854809\\n',\n",
       " 'BACKGROUND\\tEmotional eating is associated with overeating and the development of obesity .\\n',\n",
       " 'BACKGROUND\\tYet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .\\n',\n",
       " 'OBJECTIVE\\tThe aim of this study was to test if attention bias for food moderates the effect of self-reported emotional eating during sad mood ( vs neutral mood ) on actual food intake .\\n',\n",
       " 'OBJECTIVE\\tIt was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .\\n',\n",
       " 'METHODS\\tParticipants ( N = @ ) were randomly assigned to one of the two experimental mood induction conditions ( sad/neutral ) .\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read train file\n",
    "train_lines = get_lines(data_dir + \"train.txt\")\n",
    "train_lines[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore need to separate the target labels (BACKGROUND, OBJECTIVE, etc.) from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BK-9fBnR4_w"
   },
   "outputs": [],
   "source": [
    "def preprocess_text_with_line_numbers(filename):\n",
    "  \"\"\"\n",
    "  Returns a list of dictionaries containing abstract data.\n",
    "  \"\"\"\n",
    "  input_lines = get_lines(filename)\n",
    "  abstract_lines = \"\"\n",
    "  abstract_samples = []\n",
    "\n",
    "  for line in input_lines:\n",
    "    if line.startswith(\"###\"): # The id of the abstract\n",
    "      abstract_id = line\n",
    "      abstract_lines = \"\"\n",
    "    elif line.isspace(): # On a new line?\n",
    "      abstract_lines_split = abstract_lines.splitlines()\n",
    "      for abstract_line_number, abstract_line in enumerate(abstract_lines_split):\n",
    "        line_data = {}\n",
    "        target_text_split = abstract_line.split(\"\\t\")\n",
    "        line_data[\"target\"] = target_text_split[0]\n",
    "        line_data[\"text\"] = target_text_split[1].lower()\n",
    "        line_data[\"line_number\"] = abstract_line_number\n",
    "        line_data[\"total_lines\"] = len(abstract_lines_split) - 1\n",
    "        # Append dictionary\n",
    "        abstract_samples.append(line_data)\n",
    "    else:\n",
    "      abstract_lines += line\n",
    "\n",
    "  return abstract_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAC54tyXOJQ7"
   },
   "outputs": [],
   "source": [
    "train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
    "val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\")\n",
    "test_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f10LfSeOQJb",
    "outputId": "a646ade9-c2f4-4603-db03-faa65daaae34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'line_number': 0,\n",
       "  'target': 'OBJECTIVE',\n",
       "  'text': 'to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       "  'total_lines': 11},\n",
       " {'line_number': 1,\n",
       "  'target': 'METHODS',\n",
       "  'text': 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       "  'total_lines': 11}]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jfr8unlaVM3j"
   },
   "source": [
    "Let's turn the data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cEnSoP1Ph-W"
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "Vz_XZSljVaPq",
    "outputId": "d4f8597c-c685-4e44-8bad-f07e7daed8ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>a total of @ patients with primary knee oa wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>secondary outcome measures included the wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target  ... total_lines\n",
       "0  OBJECTIVE  ...          11\n",
       "1    METHODS  ...          11\n",
       "2    METHODS  ...          11\n",
       "3    METHODS  ...          11\n",
       "4    METHODS  ...          11\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KMSp8O0iVbJu",
    "outputId": "67f78ae5-5f12-48a0-9b85-7634560166c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "METHODS        59353\n",
       "RESULTS        57953\n",
       "CONCLUSIONS    27168\n",
       "BACKGROUND     21727\n",
       "OBJECTIVE      13839\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of data\n",
    "train_df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "oUvwabhdVpHV",
    "outputId": "c2c28093-065e-41e5-954c-713b5d40dcce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe053eba150>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of line lengths\n",
    "train_df[\"total_lines\"].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIorBo8dV77Y"
   },
   "source": [
    "### Get lists of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjhXFb2WV2Ry"
   },
   "outputs": [],
   "source": [
    "# Convert the lines into lists\n",
    "train_sentences = train_df[\"text\"].tolist()\n",
    "val_sentences = val_df[\"text\"].tolist()\n",
    "test_sentences = test_df[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PuJW9t6wWGBq",
    "outputId": "581e56b2-b69e-4ebd-a524-ddcfca0fc331"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       " 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       " 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
       " 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
       " 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n",
       " 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n",
       " 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n",
       " 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n",
       " 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n",
       " 'these differences remained significant at @ weeks .']"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qOzw3OZWuKo"
   },
   "source": [
    "## Make numeric labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YZJ4wtZWK5G"
   },
   "outputs": [],
   "source": [
    "# One hot encode labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYQWWzfdXDgb"
   },
   "outputs": [],
   "source": [
    "# Label encode\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
    "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
    "test_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7TPxNMeYL-c",
    "outputId": "64e4b336-24b1-49c1-def0-3b8deb4af9db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check one-hot-encoded labels\n",
    "train_labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PY0pYOwpYOco",
    "outputId": "725f4074-2239-4476-aa77-e0dceb25d27b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, ..., 4, 1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check label-encoded labels\n",
    "train_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-_7Q84jKYQJq",
    "outputId": "0ed33a50-1199-4b24-f298-403a488892f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class names back from labels\n",
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = label_encoder.classes_\n",
    "num_classes, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare our data for deep learning\n",
    "\n",
    "Create vectorization and embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at average sentence length\n",
    "sent_len = [len(sentence.split()) for sentence in train_sentences]\n",
    "avg_sent_len = np.mean(sent_len)\n",
    "avg_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution length\n",
    "plt.hist(sent_len, bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the cuoff value we will take the 95 percentile of the distribution\n",
    "np.percentile(sent_len, q=95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create text vectorizer layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 68000 # from paper\n",
    "text_vectorizer = TextVectorization(max_tokens=max_tokens,\n",
    "                                    output_sequence_length=55) # 95 percentile above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt vectorizer to training sentences\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
    "print(len(rct_20k_text_vocab))\n",
    "print(\"Five most common:\", rct_20k_text_vocab[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embed = layers.Embedding(input_dim=len(rct_20k_text_vocab),\n",
    "                               output_dim=128, # Dimension of embedding output\n",
    "                               mask_zero=True, # For variable sequence length,\n",
    "                               name=\"token_embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actually**\n",
    "\n",
    "We will use a pretrained embedding layer\n",
    "from https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "\n",
    "NOTE: USE takes care of tokenization for us.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained TensorFlow Hub Universal Sentence Encoder (USE)\n",
    "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        trainable=False,\n",
    "                                        name=\"universal_sentence_encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create character vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to split sentences into characters\n",
    "def split_chars(text):\n",
    "  return \" \".join(list(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to entire dataset\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
    "train_chars[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average character length\n",
    "char_lens = [len(sentence) for sentence in train_sentences]\n",
    "np.mean(char_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(char_lens, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_seq_char_len = int(np.percentile(char_lens, 95))\n",
    "output_seq_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOV = out of vocabulary (=unkwown)\n",
    "# Create char-level token vectorizer\n",
    "NUM_CHAR_TOKENS = len(alphabet) + 2 # add 2 for space and OOV token\n",
    "char_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS,\n",
    "                                    output_sequence_length=output_seq_char_len,\n",
    "                                    name=\"char_vectorizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt character vectorizer to training character\n",
    "char_vectorizer.adapt(train_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vocab = char_vectorizer.get_vocabulary()\n",
    "print(f\"Number of different characters: {len(char_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a character-level embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create character-level embedding layer\n",
    "char_embed = layers.Embedding(input_dim=NUM_CHAR_TOKENS,\n",
    "                              output_dim=25, # From paper\n",
    "                              mask_zero=True,\n",
    "                              name=\"char_embed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding line numbers and lengths (Positional Embeddings)\n",
    "\n",
    "We'll add the line number and the total number of lines to the set of features (positional embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at distribution of line_number\n",
    "train_df[\"line_number\"].hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority of lines have position < 15. So let's use 15 as the depth parameter when one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Tensorflow to one-hot encode line_number\n",
    "train_line_numbers_one_hot = tf.one_hot(train_df[\"line_number\"].to_numpy(), depth=15)\n",
    "val_line_numbers_one_hot = tf.one_hot(val_df[\"line_number\"].to_numpy(), depth=15)\n",
    "test_line_numbers_one_hot = tf.one_hot(test_df[\"line_number\"].to_numpy(), depth=15)\n",
    "train_line_numbers_one_hot[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at distribution of total_lines\n",
    "train_df[\"total_lines\"].hist(bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value of 20 seems to cover the majority of samples. Let's use this when hot-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Tensorflow to one-hot encode total_lines\n",
    "train_total_lines_one_hot = tf.one_hot(train_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "val_total_lines_one_hot = tf.one_hot(val_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "test_total_lines_one_hot = tf.one_hot(test_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "train_total_lines_one_hot[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "\n",
    "Pretrained token embedding + character embedding + positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a token-level model\n",
    "token_inputs = layers.Input(shape=[], dtype=tf.string, name=\"token_inputs\")\n",
    "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
    "token_outputs = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
    "# NOTE: token_outputs doesn't seem to be used!\n",
    "token_model = tf.keras.Model(inputs=token_inputs,\n",
    "                             outputs=token_embeddings)\n",
    "                            #  outputs=token_outputs)\n",
    "\n",
    "# 2. Create a character-level model\n",
    "char_inputs = layers.Input(shape=(1, ), dtype=tf.string, name=\"char_inputs\")\n",
    "char_vectors = char_vectorizer(char_inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "char_bi_lstm = layers.Bidirectional(layers.LSTM(32))(char_embeddings)\n",
    "char_model = tf.keras.Model(inputs=char_inputs,\n",
    "                            outputs=char_bi_lstm)\n",
    "\n",
    "# 3. Create model for line_number\n",
    "line_number_inputs = layers.Input(shape=(15,), dtype=tf.float32, name=\"line_number_inputs\")\n",
    "line_number_outputs = layers.Dense(32, activation=\"relu\")(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(inputs=line_number_inputs,\n",
    "                                   outputs=line_number_outputs)\n",
    "\n",
    "# 4. Create model for total_lines\n",
    "total_lines_inputs = layers.Input(shape=(20,), dtype=tf.float32, name=\"total_lines_inputs\")\n",
    "total_lines_outputs = layers.Dense(32, activation=\"relu\")(total_lines_inputs)\n",
    "total_lines_model = tf.keras.Model(inputs=total_lines_inputs,\n",
    "                                   outputs=total_lines_outputs)\n",
    "\n",
    "# 5. Combine 1 & 2\n",
    "combined_embeddings = layers.Concatenate(name=\"token_char_hybrid_embedding\")([token_model.output, char_model.output])\n",
    "z = layers.Dense(256, activation=\"relu\")(combined_embeddings)\n",
    "z = layers.Dropout(0.5)(z)\n",
    "\n",
    "# 6. Combine 3, 4, and 5\n",
    "z = layers.Concatenate(name=\"token_char_positional_embedding\")([line_number_model.output,\n",
    "                                                                total_lines_model.output,\n",
    "                                                                z])\n",
    "\n",
    "# 7. Create output layer\n",
    "output_layer = layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\")(z)\n",
    "\n",
    "# 8. Combine inputs from 1, 2, 3, 4 and outputs of 7 into model\n",
    "model = tf.keras.Model(inputs=[line_number_model.input,\n",
    "                               total_lines_model.input,\n",
    "                               token_model.input,\n",
    "                               char_model.input],\n",
    "                       outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model (label smoothing helps prevent overfitting)\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_char_token_pos_data = tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot,\n",
    "                                                                train_total_lines_one_hot,\n",
    "                                                                train_sentences,\n",
    "                                                                train_chars))\n",
    "train_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_char_token_pos_dataset = tf.data.Dataset.zip((train_char_token_pos_data, train_char_token_pos_labels))\n",
    "train_char_token_pos_dataset = train_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_char_token_pos_data = tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot,\n",
    "                                                              val_total_lines_one_hot,\n",
    "                                                              val_sentences,\n",
    "                                                              val_chars))\n",
    "val_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_char_token_pos_dataset = tf.data.Dataset.zip((val_char_token_pos_data, val_char_token_pos_labels))\n",
    "val_char_token_pos_dataset = val_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char_token_pos_dataset, val_char_token_pos_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                              patience=3,\n",
    "                                              restore_best_weights=True,\n",
    "                                              verbose=0)\n",
    "# NOTE: need to train on whole dataset (not 10%), but this fails with early_stop\n",
    "# Stopped at epoch 20 (about 1h30)\n",
    "history_model = model.fit(train_char_token_pos_dataset,\n",
    "                          epochs=100, # Bump up to 100 when using early stop\n",
    "                          # steps_per_epoch=int(0.1 * len(train_char_token_pos_dataset)),\n",
    "                          callbacks=[early_stop],\n",
    "                          validation_data=val_char_token_pos_dataset,\n",
    "                          validation_steps=int(0.1 * len(val_char_token_pos_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(val_char_token_pos_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred_probs = model.predict(val_char_token_pos_dataset)\n",
    "model_preds = tf.argmax(model_pred_probs, axis=1)\n",
    "# Code used to save preds\n",
    "# pd.DataFrame({\"preds\": model_preds.numpy(), \"truth\": val_labels_encoded}).to_csv(\"preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_results(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
    "  Args:\n",
    "      y_true: true labels in the form of a 1D array\n",
    "      y_pred: predicted labels in the form of a 1D array\n",
    "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
    "  \"\"\"\n",
    "  # Calculate model accuracy\n",
    "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "  # Calculate model precision, recall and f1 score using \"weighted average\n",
    "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "  model_results = {\"accuracy\": model_accuracy,\n",
    "                  \"precision\": model_precision,\n",
    "                  \"recall\": model_recall,\n",
    "                  \"f1\": model_f1}\n",
    "  return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                  y_pred=model_preds)\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "milestone2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
