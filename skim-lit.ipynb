{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipt1WLy5J16w"
   },
   "source": [
    "# SkimLit\n",
    "\n",
    "NLP model to make reading medical abstracts easier.\n",
    "\n",
    "To do this, we will implement the deep learning model described in https://arxiv.org/pdf/1612.05251.pdf\n",
    "\n",
    "Project done on Google Colab (Tesla T4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqRWqPs6KVJp"
   },
   "source": [
    "## Get the data\n",
    "\n",
    "The data is publicly available on GitHub so let's download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQhWcP2tKJJA",
    "outputId": "de88e239-dd16-47e2-f092-4b347d283ced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mPubMed_200k_RCT\u001b[m\u001b[m\r\n",
      "\u001b[34mPubMed_200k_RCT_numbers_replaced_with_at_sign\u001b[m\u001b[m\r\n",
      "\u001b[34mPubMed_20k_RCT\u001b[m\u001b[m\r\n",
      "\u001b[34mPubMed_20k_RCT_numbers_replaced_with_at_sign\u001b[m\u001b[m\r\n",
      "README.md\r\n"
     ]
    }
   ],
   "source": [
    "download_data = False\n",
    "if download_data:\n",
    "    !git clone https://github.com/Franck-Dernoncourt/pubmed-rct\n",
    "!ls pubmed-rct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UczyWLTUK-UT",
    "outputId": "6453cd8c-3027-4153-c521-c40763adea40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.txt   test.txt  train.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have shorter compute times, we will use the PubMed_20k_RCT dataset, which is a subset (10%) of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h1wiF_QFLLTh"
   },
   "outputs": [],
   "source": [
    "# Let's work on the smaller dataset (20k instead of 200k)\n",
    "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3TVslibL9VE",
    "outputId": "69e0982a-9b20-4882-a00d-c35dababb79b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt',\n",
       " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt',\n",
       " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjmHe-uxMYcT"
   },
   "source": [
    "## Preprocess the data\n",
    "\n",
    "For the deep learning model to run, we will need to transform text into numbers. Hovewer, before this preprocessing step, let's see what the data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "duyRXLKoMEDk"
   },
   "outputs": [],
   "source": [
    "# Function to read text\n",
    "def get_lines(filename):\n",
    "    \"\"\"\n",
    "    Reads filename and returns the lines as a list.\n",
    "\n",
    "    Args:\n",
    "    filename: path to text file.\n",
    "\n",
    "    Returns:\n",
    "    A list of strings with one string per line.\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\") as ifs:\n",
    "        return ifs.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pFclj57WM-oS",
    "outputId": "b277ef8d-e96f-416a-87eb-73e747fdd133"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['###24293578\\n',\n",
       " 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n',\n",
       " 'METHODS\\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n',\n",
       " 'METHODS\\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\\n',\n",
       " 'METHODS\\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\\n',\n",
       " 'METHODS\\tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .\\n',\n",
       " 'METHODS\\tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .\\n',\n",
       " 'RESULTS\\tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .\\n',\n",
       " 'RESULTS\\tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .\\n',\n",
       " 'RESULTS\\tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .\\n',\n",
       " 'RESULTS\\tThese differences remained significant at @ weeks .\\n',\n",
       " 'RESULTS\\tThe Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .\\n',\n",
       " 'CONCLUSIONS\\tLow-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee OA ( ClinicalTrials.gov identifier NCT@ ) .\\n',\n",
       " '\\n',\n",
       " '###24854809\\n',\n",
       " 'BACKGROUND\\tEmotional eating is associated with overeating and the development of obesity .\\n',\n",
       " 'BACKGROUND\\tYet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .\\n',\n",
       " 'OBJECTIVE\\tThe aim of this study was to test if attention bias for food moderates the effect of self-reported emotional eating during sad mood ( vs neutral mood ) on actual food intake .\\n',\n",
       " 'OBJECTIVE\\tIt was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .\\n',\n",
       " 'METHODS\\tParticipants ( N = @ ) were randomly assigned to one of the two experimental mood induction conditions ( sad/neutral ) .\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read train file\n",
    "train_lines = get_lines(data_dir + \"train.txt\")\n",
    "train_lines[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to separate the target labels (BACKGROUND, OBJECTIVE, etc.) from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_BK-9fBnR4_w"
   },
   "outputs": [],
   "source": [
    "def preprocess_text_with_line_numbers(filename):\n",
    "    \"\"\"\n",
    "    Returns a list of dictionaries containing abstract data.\n",
    "    \"\"\"\n",
    "    input_lines = get_lines(filename)\n",
    "    abstract_lines = \"\"\n",
    "    abstract_samples = []\n",
    "\n",
    "    for line in input_lines:\n",
    "        if line.startswith(\"###\"): # The id of the abstract\n",
    "            abstract_id = line\n",
    "            abstract_lines = \"\"\n",
    "        elif line.isspace(): # On a new line?\n",
    "            abstract_lines_split = abstract_lines.splitlines()\n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_lines_split):\n",
    "                line_data = {}\n",
    "                target_text_split = abstract_line.split(\"\\t\")\n",
    "                line_data[\"target\"] = target_text_split[0]\n",
    "                line_data[\"text\"] = target_text_split[1].lower()\n",
    "                line_data[\"line_number\"] = abstract_line_number\n",
    "                line_data[\"total_lines\"] = len(abstract_lines_split) - 1\n",
    "                # Append dictionary\n",
    "                abstract_samples.append(line_data)\n",
    "        else:\n",
    "            abstract_lines += line\n",
    "\n",
    "    return abstract_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eAC54tyXOJQ7"
   },
   "outputs": [],
   "source": [
    "train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
    "val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\")\n",
    "test_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f10LfSeOQJb",
    "outputId": "a646ade9-c2f4-4603-db03-faa65daaae34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'target': 'OBJECTIVE',\n",
       "  'text': 'to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 11}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jfr8unlaVM3j"
   },
   "source": [
    "We can now turn these lists of dictionaries into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6cEnSoP1Ph-W"
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "Vz_XZSljVaPq",
    "outputId": "d4f8597c-c685-4e44-8bad-f07e7daed8ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>a total of @ patients with primary knee oa wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>secondary outcome measures included the wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text  line_number  \\\n",
       "0  OBJECTIVE  to investigate the efficacy of @ weeks of dail...            0   \n",
       "1    METHODS  a total of @ patients with primary knee oa wer...            1   \n",
       "2    METHODS  outcome measures included pain reduction and i...            2   \n",
       "3    METHODS  pain was assessed using the visual analog pain...            3   \n",
       "4    METHODS  secondary outcome measures included the wester...            4   \n",
       "\n",
       "   total_lines  \n",
       "0           11  \n",
       "1           11  \n",
       "2           11  \n",
       "3           11  \n",
       "4           11  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now have a look at the distributions of training labels and abstract length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KMSp8O0iVbJu",
    "outputId": "67f78ae5-5f12-48a0-9b85-7634560166c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "METHODS        59353\n",
       "RESULTS        57953\n",
       "CONCLUSIONS    27168\n",
       "BACKGROUND     21727\n",
       "OBJECTIVE      13839\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of labels in the training set\n",
    "train_df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHODS is the most common, while OBJECTIVE the least common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "oUvwabhdVpHV",
    "outputId": "c2c28093-065e-41e5-954c-713b5d40dcce"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of line lengths\n",
    "train_df[\"total_lines\"].plot.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the majority of abstracts is between 6 to 15 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIorBo8dV77Y"
   },
   "source": [
    "Finally, for building our model, it will be useful to have the columns of our dataframes as lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sjhXFb2WV2Ry"
   },
   "outputs": [],
   "source": [
    "# Convert the dataframe columns into lists\n",
    "train_sentences = train_df[\"text\"].tolist()\n",
    "val_sentences = val_df[\"text\"].tolist()\n",
    "test_sentences = test_df[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PuJW9t6wWGBq",
    "outputId": "581e56b2-b69e-4ebd-a524-ddcfca0fc331"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       " 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       " 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
       " 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
       " 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n",
       " 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n",
       " 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n",
       " 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n",
       " 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n",
       " 'these differences remained significant at @ weeks .']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qOzw3OZWuKo"
   },
   "source": [
    "## Prepare numeric labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use both one-hot encoding and label encoding. One-hot encoding is required when using CategoricalCrossentropy as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1YZJ4wtZWK5G"
   },
   "outputs": [],
   "source": [
    "# One hot encode labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vYQWWzfdXDgb"
   },
   "outputs": [],
   "source": [
    "# Label encode\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
    "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
    "test_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7TPxNMeYL-c",
    "outputId": "64e4b336-24b1-49c1-def0-3b8deb4af9db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check one-hot-encoded labels\n",
    "train_labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PY0pYOwpYOco",
    "outputId": "725f4074-2239-4476-aa77-e0dceb25d27b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, ..., 4, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check label-encoded labels\n",
    "train_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-_7Q84jKYQJq",
    "outputId": "0ed33a50-1199-4b24-f298-403a488892f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class names back from labels\n",
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = label_encoder.classes_\n",
    "num_classes, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for deep learning\n",
    "\n",
    "The model will have three components:\n",
    "\n",
    "1. A token-level model.\n",
    "2. A character-level model.\n",
    "3. Two positional models: the first one for the line number within the abstract and the second one for the total number of lines in the abstract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token-level layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the token-level model, we will use a pretrained embedding layer (the Universal Sentence Encoder, or USE) available at https://tfhub.dev/google/universal-sentence-encoder/4.\n",
    "\n",
    "NOTE: The USE takes care of the tokenization for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained TensorFlow Hub Universal Sentence Encoder (USE)\n",
    "# NOTE: this is ~1Gb and takes a while to download\n",
    "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        trainable=False,\n",
    "                                        name=\"universal_sentence_encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-level layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the character level model we first need to split the sentences into characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to split sentences into characters\n",
    "def split_chars(text):\n",
    "    return \" \".join(list(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t o   i n v e s t i g a t e   t h e   e f f i c a c y   o f   @   w e e k s   o f   d a i l y   l o w - d o s e   o r a l   p r e d n i s o l o n e   i n   i m p r o v i n g   p a i n   ,   m o b i l i t y   ,   a n d   s y s t e m i c   l o w - g r a d e   i n f l a m m a t i o n   i n   t h e   s h o r t   t e r m   a n d   w h e t h e r   t h e   e f f e c t   w o u l d   b e   s u s t a i n e d   a t   @   w e e k s   i n   o l d e r   a d u l t s   w i t h   m o d e r a t e   t o   s e v e r e   k n e e   o s t e o a r t h r i t i s   (   o a   )   .',\n",
       " 'a   t o t a l   o f   @   p a t i e n t s   w i t h   p r i m a r y   k n e e   o a   w e r e   r a n d o m i z e d   @ : @   ;   @   r e c e i v e d   @   m g / d a y   o f   p r e d n i s o l o n e   a n d   @   r e c e i v e d   p l a c e b o   f o r   @   w e e k s   .',\n",
       " 'o u t c o m e   m e a s u r e s   i n c l u d e d   p a i n   r e d u c t i o n   a n d   i m p r o v e m e n t   i n   f u n c t i o n   s c o r e s   a n d   s y s t e m i c   i n f l a m m a t i o n   m a r k e r s   .',\n",
       " 'p a i n   w a s   a s s e s s e d   u s i n g   t h e   v i s u a l   a n a l o g   p a i n   s c a l e   (   @ - @   m m   )   .',\n",
       " 's e c o n d a r y   o u t c o m e   m e a s u r e s   i n c l u d e d   t h e   w e s t e r n   o n t a r i o   a n d   m c m a s t e r   u n i v e r s i t i e s   o s t e o a r t h r i t i s   i n d e x   s c o r e s   ,   p a t i e n t   g l o b a l   a s s e s s m e n t   (   p g a   )   o f   t h e   s e v e r i t y   o f   k n e e   o a   ,   a n d   @ - m i n   w a l k   d i s t a n c e   (   @ m w d   )   .',\n",
       " 's e r u m   l e v e l s   o f   i n t e r l e u k i n   @   (   i l - @   )   ,   i l - @   ,   t u m o r   n e c r o s i s   f a c t o r   (   t n f   )   -   ,   a n d   h i g h - s e n s i t i v i t y   c - r e a c t i v e   p r o t e i n   (   h s c r p   )   w e r e   m e a s u r e d   .',\n",
       " 't h e r e   w a s   a   c l i n i c a l l y   r e l e v a n t   r e d u c t i o n   i n   t h e   i n t e r v e n t i o n   g r o u p   c o m p a r e d   t o   t h e   p l a c e b o   g r o u p   f o r   k n e e   p a i n   ,   p h y s i c a l   f u n c t i o n   ,   p g a   ,   a n d   @ m w d   a t   @   w e e k s   .',\n",
       " 't h e   m e a n   d i f f e r e n c e   b e t w e e n   t r e a t m e n t   a r m s   (   @   %   c i   )   w a s   @   (   @ - @   @   )   ,   p   <   @   ;   @   (   @ - @   @   )   ,   p   <   @   ;   @   (   @ - @   @   )   ,   p   <   @   ;   a n d   @   (   @ - @   @   )   ,   p   <   @   ,   r e s p e c t i v e l y   .',\n",
       " 'f u r t h e r   ,   t h e r e   w a s   a   c l i n i c a l l y   r e l e v a n t   r e d u c t i o n   i n   t h e   s e r u m   l e v e l s   o f   i l - @   ,   i l - @   ,   t n f   -   ,   a n d   h s c r p   a t   @   w e e k s   i n   t h e   i n t e r v e n t i o n   g r o u p   w h e n   c o m p a r e d   t o   t h e   p l a c e b o   g r o u p   .',\n",
       " 't h e s e   d i f f e r e n c e s   r e m a i n e d   s i g n i f i c a n t   a t   @   w e e k s   .']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply function to entire dataset\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
    "train_chars[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149.3662574983337"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average character length\n",
    "char_lens = [len(sentence) for sentence in train_sentences]\n",
    "np.mean(char_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(char_lens, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seq_char_len = int(np.percentile(char_lens, 95))\n",
    "output_seq_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the character-level vectorizer and adapt it to our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOV = out of vocabulary (i.e. unkwown)\n",
    "# Create char-level token vectorizer\n",
    "NUM_CHAR_TOKENS = len(alphabet) + 2 # add 2 for space and OOV token\n",
    "char_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS,\n",
    "                                    output_sequence_length=output_seq_char_len,\n",
    "                                    name=\"char_vectorizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt character vectorizer to training set (characters)\n",
    "char_vectorizer.adapt(train_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different characters: 28\n"
     ]
    }
   ],
   "source": [
    "char_vocab = char_vectorizer.get_vocabulary()\n",
    "print(f\"Number of different characters: {len(char_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create the character-level embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create character-level embedding layer\n",
    "char_embed = layers.Embedding(input_dim=NUM_CHAR_TOKENS,\n",
    "                              output_dim=25, # From paper\n",
    "                              mask_zero=True,\n",
    "                              name=\"char_embed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings\n",
    "\n",
    "For the last two components of the model, we will need the line number and the total number of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's look at distribution of line_number\n",
    "train_df[\"line_number\"].hist(bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the lines have a position < 15. So we will use 15 as the depth parameter when one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 15), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Tensorflow to one-hot encode line_number\n",
    "train_line_numbers_one_hot = tf.one_hot(train_df[\"line_number\"].to_numpy(), depth=15)\n",
    "val_line_numbers_one_hot = tf.one_hot(val_df[\"line_number\"].to_numpy(), depth=15)\n",
    "test_line_numbers_one_hot = tf.one_hot(test_df[\"line_number\"].to_numpy(), depth=15)\n",
    "train_line_numbers_one_hot[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the previous steps for the total number of lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's look at distribution of total_lines\n",
    "train_df[\"total_lines\"].hist(bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value of 20 seems to cover the majority of samples. Let's use this value when one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 20), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Tensorflow to one-hot encode total_lines\n",
    "train_total_lines_one_hot = tf.one_hot(train_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "val_total_lines_one_hot = tf.one_hot(val_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "test_total_lines_one_hot = tf.one_hot(test_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "train_total_lines_one_hot[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "We have now all the building blocks to define the final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a token-level model\n",
    "token_inputs = layers.Input(shape=[], dtype=tf.string, name=\"token_inputs\")\n",
    "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
    "token_outputs = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
    "# NOTE: token_outputs doesn't seem to be used!\n",
    "token_model = tf.keras.Model(inputs=token_inputs,\n",
    "                             outputs=token_embeddings)\n",
    "                            #  outputs=token_outputs)\n",
    "\n",
    "# 2. Create a character-level model\n",
    "char_inputs = layers.Input(shape=(1, ), dtype=tf.string, name=\"char_inputs\")\n",
    "char_vectors = char_vectorizer(char_inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "char_bi_lstm = layers.Bidirectional(layers.LSTM(32))(char_embeddings)\n",
    "char_model = tf.keras.Model(inputs=char_inputs,\n",
    "                            outputs=char_bi_lstm)\n",
    "\n",
    "# 3. Create model for line_number\n",
    "line_number_inputs = layers.Input(shape=(15,), dtype=tf.float32, name=\"line_number_inputs\")\n",
    "line_number_outputs = layers.Dense(32, activation=\"relu\")(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(inputs=line_number_inputs,\n",
    "                                   outputs=line_number_outputs)\n",
    "\n",
    "# 4. Create model for total_lines\n",
    "total_lines_inputs = layers.Input(shape=(20,), dtype=tf.float32, name=\"total_lines_inputs\")\n",
    "total_lines_outputs = layers.Dense(32, activation=\"relu\")(total_lines_inputs)\n",
    "total_lines_model = tf.keras.Model(inputs=total_lines_inputs,\n",
    "                                   outputs=total_lines_outputs)\n",
    "\n",
    "# 5. Combine 1 and 2\n",
    "combined_embeddings = layers.Concatenate(name=\"token_char_hybrid_embedding\")([token_model.output, char_model.output])\n",
    "z = layers.Dense(256, activation=\"relu\")(combined_embeddings)\n",
    "z = layers.Dropout(0.5)(z)\n",
    "\n",
    "# 6. Combine 3, 4, and 5\n",
    "z = layers.Concatenate(name=\"token_char_positional_embedding\")([line_number_model.output,\n",
    "                                                                total_lines_model.output,\n",
    "                                                                z])\n",
    "\n",
    "# 7. Create output layer\n",
    "output_layer = layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\")(z)\n",
    "\n",
    "# 8. Combine inputs from 1, 2, 3, 4 and outputs of 7 into final model\n",
    "model = tf.keras.Model(inputs=[line_number_model.input,\n",
    "                               total_lines_model.input,\n",
    "                               token_model.input,\n",
    "                               char_model.input],\n",
    "                       outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_inputs (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_vectorizer (TextVectorizat (None, 290)          0           char_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "token_inputs (InputLayer)       [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embed (Embedding)          (None, 290, 25)      1750        char_vectorizer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "universal_sentence_encoder (Ker (None, 512)          256797824   token_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 64)           14848       char_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "token_char_hybrid_embedding (Co (None, 576)          0           universal_sentence_encoder[0][0] \n",
      "                                                                 bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "line_number_inputs (InputLayer) [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "total_lines_inputs (InputLayer) [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          147712      token_char_hybrid_embedding[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           512         line_number_inputs[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           672         total_lines_inputs[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "token_char_positional_embedding (None, 320)          0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 5)            1605        token_char_positional_embedding[0\n",
      "==================================================================================================\n",
      "Total params: 256,964,923\n",
      "Trainable params: 167,099\n",
      "Non-trainable params: 256,797,824\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model (label smoothing helps prevent overfitting)\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_char_token_pos_data = tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot,\n",
    "                                                                train_total_lines_one_hot,\n",
    "                                                                train_sentences,\n",
    "                                                                train_chars))\n",
    "train_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_char_token_pos_dataset = tf.data.Dataset.zip((train_char_token_pos_data, train_char_token_pos_labels))\n",
    "train_char_token_pos_dataset = train_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_char_token_pos_data = tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot,\n",
    "                                                              val_total_lines_one_hot,\n",
    "                                                              val_sentences,\n",
    "                                                              val_chars))\n",
    "val_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_char_token_pos_dataset = tf.data.Dataset.zip((val_char_token_pos_data, val_char_token_pos_labels))\n",
    "val_char_token_pos_dataset = val_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: (((None, 15), (None, 20), (None,), (None,)), (None, 5)), types: ((tf.float32, tf.float32, tf.string, tf.string), tf.float64)>,\n",
       " <PrefetchDataset shapes: (((None, 15), (None, 20), (None,), (None,)), (None, 5)), types: ((tf.float32, tf.float32, tf.string, tf.string), tf.float64)>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_char_token_pos_dataset, val_char_token_pos_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False\n",
    "if train:\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                                  patience=3,\n",
    "                                                  restore_best_weights=True,\n",
    "                                                  verbose=0)\n",
    "    # NOTE: need to train on whole dataset (not 10%), but this fails with early_stop\n",
    "    # Stopped at epoch 20 (about 1h30)\n",
    "    history_model = model.fit(train_char_token_pos_dataset,\n",
    "                              epochs=100, # Bump up to 100 when using early stop\n",
    "                              # steps_per_epoch=int(0.1 * len(train_char_token_pos_dataset)),\n",
    "                              callbacks=[early_stop],\n",
    "                              validation_data=val_char_token_pos_dataset,\n",
    "                              validation_steps=int(0.1 * len(val_char_token_pos_dataset)))\n",
    "    \n",
    "    model.evaluate(val_char_token_pos_dataset)\n",
    "\n",
    "    model_pred_probs = model.predict(val_char_token_pos_dataset)\n",
    "    model_preds = tf.argmax(model_pred_probs, axis=1)\n",
    "\n",
    "    res = pd.DataFrame({\"preds\": model_preds.numpy(), \"truth\": val_labels_encoded})\n",
    "    res.to_csv(\"results/preds.csv\", index=False)\n",
    "else:\n",
    "    res = pd.read_csv(\"results/preds.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_results(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
    "    Args:\n",
    "      y_true: true labels in the form of a 1D array\n",
    "      y_pred: predicted labels in the form of a 1D array\n",
    "    Returns a dictionary of accuracy, precision, recall, f1-score.\n",
    "    \"\"\"\n",
    "    # Calculate model accuracy\n",
    "    model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    # Calculate model precision, recall and f1 score using weighted average\n",
    "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    model_results = {\"accuracy\": model_accuracy,\n",
    "                     \"precision\": model_precision,\n",
    "                     \"recall\": model_recall,\n",
    "                     \"f1\": model_f1}\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 87.18390043691248,\n",
       " 'precision': 0.8722665244496874,\n",
       " 'recall': 0.8718390043691249,\n",
       " 'f1': 0.8691866359586281}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results = calculate_results(y_true=res[\"truth\"],\n",
    "                                  y_pred=res[\"preds\"])\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30207</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30208</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30209</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30210</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30211</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30212 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       preds  truth\n",
       "0          0      0\n",
       "1          0      0\n",
       "2          0      3\n",
       "3          2      2\n",
       "4          2      2\n",
       "...      ...    ...\n",
       "30207      4      4\n",
       "30208      4      4\n",
       "30209      4      4\n",
       "30210      4      1\n",
       "30211      1      1\n",
       "\n",
       "[30212 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "milestone2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
